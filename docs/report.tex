\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{url}
\usepackage[breaklinks=true]{hyperref}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{booktabs}

\geometry{margin=2.5cm}

\title{Report Kernel Rebooters: Vision Division}
\author{ 
	Marco Carraro \\ {\small \texttt{marco.carraro.23@studenti.unipd.it}} \and
	Luca Pellegrini \\ {\small \texttt{luca.pellegrini.6@studenti.unipd.it}} \and
	Francesco Vezzani \\ {\small \texttt{francesco.vezzani.1@studenti.unipd.it}}
}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section{Introduction}
	The overall goal of this project is to build a complete computer vision pipeline that can
	classify flower images and support further analysis tasks, such as health-state estimation
	and performance evaluation. In practical terms, the system is designed around a clear
	workflow: images are loaded from a structured dataset, visual descriptors are extracted,
	test images are compared against training references, and final predictions are produced.

	This report remains a shared team document, so section titles for all project components
	are kept even when implementation details are still pending. The current revision documents
	the HOG and BoW modules according to their latest simplified implementation: both modules
	now expose compact extractor interfaces focused only on feature extraction, vocabulary
	creation (for BoW), and descriptor matching.

	\section{Preprocessing}
	\textit{TODO}

	\section{SIFT}
	\subsection{Code Structure Decision}
	SIFT logic is organized into:
	\begin{itemize}
		\item \texttt{SIFTExtractor} (\texttt{include/sift.h}, \texttt{src/sift.cpp}) for 
		low-level feature extraction, descriptor matching, and match filtering operations.
		\item \texttt{sift\_processing} module (\texttt{include/sift\_processing.h}, 
		\texttt{src/sift\_processing.cpp}) for dataset-level training and testing workflows.
	\end{itemize}

	This separation maintains architectural consistency with other modules: the extractor class 
	handles algorithmic operations while the processing module orchestrates dataset traversal, 
	descriptor aggregation, and prediction logic. As a result, \texttt{main.cpp} remains focused 
	on high-level coordination, and the SIFT components can be independently tested, optimized, or replaced.

	The \texttt{SIFTExtractor} interface exposes essential operations: \texttt{extract(...)}, 
	\texttt{matchDescriptors(...)}, \texttt{filterMatches(...)}, and \texttt{matchAndFilter(...)}. 
	Timing instrumentation is included to support performance analysis during development and evaluation phases.

	\subsection{Feature Extraction Choices}
	\begin{itemize}
		\item Input images are in grayscale to focus on gradient information rather than 
		color variation.
		\item SIFT parameters use OpenCV defaults with optional tuning: configurable number of features, 
		octave layers, contrast threshold, edge threshold, and Gaussian sigma.
		\item The default configuration (\texttt{nfeatures=0}) extracts all detected keypoints, providing 
		maximum detail for the training phase.
	\end{itemize}

	The decision to use grayscale processing aligns with SIFT's design philosophy: the algorithm was 
	originally developed for intensity-based matching, and color information typically adds minimal 
	discriminative value while increasing computational cost.

	\subsection{Training Pipeline}
	The training workflow follows a clear sequence:
	\begin{enumerate}
		\item Extract SIFT descriptors from all healthy and diseased (if \texttt{use\_diseased} flag is set) training images.
		\item Aggregate descriptors per class using vertical concatenation (\texttt{cv::vconcat}).
		\item Store the combined descriptor matrices in a class-indexed map for efficient test-time retrieval.
	\end{enumerate}

	This pipeline choice reflects a bag-of-features approach: all descriptors from a given class 
	are pooled together without explicit spatial structure. The rationale is simplicity and robustness
	the classifier relies on the statistical distribution of local features rather than their precise 
	geometric arrangement.

	An important implementation detail is the use of \texttt{cv::vconcat} for descriptor combination, 
	which is significantly faster than iterative concatenation and produces a single contiguous memory 
	block per class.

	\subsection{Matching and Classification Strategy}
	For each test image:
	\begin{enumerate}
		\item Extract SIFT descriptors.
		\item Match against each class's descriptor pool using FLANN-based matching.
		\item Filter matches based on distance threshold: \( d_{match} < threshold \times d_{min} \).
		\item Classify as the class with the maximum number of good matches.
	\end{enumerate}

	\subsection{Matcher Configuration}
	Two matcher implementations were tested:
	\begin{itemize}
		\item \texttt{BFMatcher} with L2 norm: exhaustive search, guaranteed optimal matches, but 
		slower for large descriptor sets (around 25 minutes for a classification).
		\item \texttt{FlannBasedMatcher}: approximate nearest neighbor search using KD-trees, 
		significantly faster with same accuracy (around 5 minutes for a classification).
	\end{itemize}

	The FLANN matcher was selected as the default after benchmarking showed 5x speedup with 
	negligible impact on classification accuracy for this dataset.

	\subsection{Robustness Choices}
	\begin{itemize}
		\item Empty image check before extraction with early return and error message.
		\item Empty descriptor validation before matching operations.
		\item Graceful handling of images with zero detected keypoints (skipped in testing loop).
		\item Timing instrumentation for extraction and matching phases to identify performance bottlenecks.
	\end{itemize}

	\subsection{Performance Considerations}
	The implementation includes several deliberate trade-offs:
	\begin{itemize}
		\item \textbf{Descriptor aggregation}: All training descriptors are stored in memory. 
		For very large datasets (thousands of images per class), this could exceed available memory. 
		A potential mitigation would be descriptor subsampling or clustering-based aggregation.
		\item \textbf{Exhaustive matching}: Each test descriptor is matched against all training 
		descriptors for a given class. This scales as \(O(N_{test} \times N_{train})\). FLANN provides 
		partial relief.
	\end{itemize}

	These trade-offs were chosen to maintain code clarity and establish a functional baseline before 
	introducing more complex optimization strategies.

	\subsection{Tested Configurations}
	Several parameters were evaluated during development:
	\begin{itemize}
		\item \textbf{Use of diseased descriptors}: Including diseased training samples improved 
		classification accuracy.
		\item \textbf{Distance threshold}: Values between 1.5 and 2.5 were tested for the ratio 
		test-inspired filtering, with 2.0 providing a good balance between precision and recall.
		\item \textbf{Matcher choice}: FLANN-based matching provided a significant speedup with 
		no loss in accuracy compared to brute-force matching.
		\item \textbf{Nearest Neighbor Distance Ratio}: The original Lowe's ratio test was implemented 
		but found to be too restrictive for this dataset, leading to very few matches. 
		The distance-based thresholding approach provided a more flexible filtering mechanism that 
		retained more valid matches while still reducing false positives.
		\item \textbf{Limits in train descriptor}: To mitigate the number of descriptors difference 
		between classes, a maximum number of descriptors per class was tested (first 15000, then 30000). 
		The sampling of those descriptors was done in multiple ways: random sampling, uniform sampling 
		and k-means clustering. The best results were obtained with random sampling but this method was 
		not consistent across runs. The other methods did not provide any improvement in accuracy and, 
		expecially k-means was very time consuming. At the end, there is non limit on the number of train 
		descriptors.
	\end{itemize}

	The final configuration selected for the evaluation phase was FLANN-based matching with a distance 
	threshold of 2.0 and no limit on the number of training descriptors, as this provided the best overall 
	accuracy while maintaining reasonable processing time.
	
	\begin{table}[H]
		\centering
		\medskip
		\resizebox{\textwidth}{!}{
		\begin{tabular}{llccclccccccc}
			\toprule
			\textbf{Matcher} & \textbf{Dataset} & \textbf{Thresh.} & \textbf{Train Descriptor} & \textbf{Time} & \textbf{Acc.} & \textbf{Daisy} & \textbf{Dand.} & \textbf{Rose} & \textbf{Sunf.} & \textbf{Tulip} & \textbf{NoFl.} \\
			\midrule 
			BF & Full & 2,5 & NULL & 26:15:996 & 20,31\% & 0,00\% & 41,67\% & 8,33\% & 16,67\% & 41,67\% & 0,00\% \\
			BF & Healthy & 2,5 & NULL & 03:04:159 & 15,63\% & 8,33\% & 41,67\% & 0,00\% & 25,00\% & 8,33\% & 0,00\% \\
			FLANN & Full & 2,5 & NULL & 06:54:950 & 20,31\% & 0,00\% & 41,67\% & 8,33\% & 16,67\% & 41,67\% & 0,00\% \\
			FLANN & Full & 2,0 & NULL & 05:25:163 & 23,44\% & 8,33\% & 41,67\% & 16,67\% & 16,67\% & 41,67\% & 0,00\% \\
			FLANN & Full & 2,5 & 15k (rnd sample) & 04:17:885 & 20,31\% & 0,00\% & 33,33\% & 8,33\% & 25,00\% & 41,67\% & 0,00\% \\
			FLANN & Full & 1,5 & 15k (rnd sample) & 03:57:923 & 25,00\% & 16,67\% & 41,67\% & 8,33\% & 33,33\% & 33,33\% & 0,00\% \\
			FLANN & Full & 1,5 & NULL & 05:29:062 & 18,75\% & 16,67\% & 33,33\% & 8,33\% & 25,00\% & 16,67\% & 0,00\% \\
			FLANN+NNDR & Full & 0,8 & NULL & 05:33:180 & 17,19\% & 25,00\% & 16,67\% & 8,33\% & 16,67\% & 25,00\% & 0,00\% \\
			FLANN+NNDR & Full & 0,7 & NULL & 05:23:491 & 23,44\% & 33,33\% & 33,33\% & 0,00\% & 16,67\% & 41,67\% & 0,00\% \\
			FLANN+NNDR & Full & 0,7 & 15k (unif sample) & 02:59:029 & 20,31\% & 41,67\% & 16,67\% & 8,33\% & 33,33\% & 8,33\% & 0,00\% \\
			FLANN+NNDR & Full & 0,7 & 15k (k-means) & $>$30' train & 18,75\% & 41,67\% & 25,00\% & 0,00\% & 25,00\% & 8,33\% & 0,00\% \\
			FLANN & Full & 2,0 & 30k (unif sample) & 04:02:434 & 20,31\% & 8,33\% & 41,67\% & 8,33\% & 25,00\% & 25,00\% & 0,00\% \\
			\bottomrule
		\end{tabular}
		}
		\caption{SIFT performance evaluation}
		\label{tab:SIFT_results}
	\end{table}

	\section{SURF}
	\textit{TODO}

	\section{ORB}
	\textit{TODO}

	\section{Template Matching}
	\textit{TODO}

	\section{HOG}
	\subsection{Objective}
	The HOG branch was introduced to add a global shape-and-gradient based descriptor that is
	easy to interpret and relatively stable in many real-world image conditions. The idea is
	simple: each test image is converted into one feature vector, then compared with feature
	vectors extracted from training images. The predicted class is taken from the nearest
	training sample.

	From an engineering perspective, the objective was not to create a heavily optimized model,
	but to produce a transparent baseline that is easy to debug, easy to validate, and fully
	consistent with the project architecture.

	\subsection{Code Structure Decision}
	HOG logic is split into:
	\begin{itemize}
		\item \texttt{HOGExtractor} (\texttt{include/hog.h}, \texttt{src/hog.cpp}) for low-level feature extraction and distance computation.
		\item \texttt{hog(...)} wrapper (\texttt{src/matching.cpp}) for dataset-level loop and prediction print.
	\end{itemize}
	
	This separation is important because it keeps responsibilities clean:
	\texttt{HOGExtractor} handles only algorithmic operations, while \texttt{hog(...)} handles
	dataset traversal and prediction flow. As a result, \texttt{main.cpp} remains focused on
	orchestration, and the module can be reused or replaced with minimal impact on the rest
	of the system.

	In the current version, \texttt{HOGExtractor} was simplified to a minimal API:
	\texttt{extract(...)} returns \texttt{bool} and \texttt{matchDescriptors(...)} returns the
	L2 distance. Timing fields and related getters were removed because they were not used by
	the application flow.

	\subsection{Feature Extraction Choices}
	\begin{itemize}
		\item Input image is converted to grayscale if needed.
		\item The image is resized to a fixed window (\texttt{64x128}).
		\item HOG parameters are basic OpenCV defaults: block size \texttt{16x16}, block stride \texttt{8x8}, cell size \texttt{8x8}, 9 bins.
	\end{itemize}
	
	The fixed resize step is a key practical decision: HOG vectors can only be compared
	directly when they share the same dimensionality. Using a fixed window gives deterministic
	descriptor length and avoids shape-dependent edge cases in matching.
	
	\subsection{Matching Choices}
	\begin{itemize}
		\item For each test descriptor, all train descriptors are scanned.
		\item Similarity score is Euclidean distance (L2).
		\item Predicted label is the train sample with minimum distance.
	\end{itemize}
	
	This results in a straightforward nearest-neighbor baseline:
	\[
	\hat{y}(x) = \arg\min_{i} \lVert h(x) - h(x_i^{train}) \rVert_2
	\]
	where \(h(\cdot)\) is the HOG descriptor.

	The benefit of this choice is interpretability: every prediction can be traced back to one
	specific training sample and one explicit distance value, which is very useful during
	qualitative inspection.
	
	\subsection{Robustness Choices}
	\begin{itemize}
		\item Empty input image check in \texttt{extract(...)}.
		\item Empty or incompatible descriptor check before matching.
		\item Sample is skipped by the wrapper when \texttt{extract(...)} returns \texttt{false}.
	\end{itemize}
	
	\subsection{Trade-off}
	The implementation is intentionally simple and readable, but the exhaustive comparison
	step has cost \(O(N_{test}\cdot N_{train})\). This is acceptable as a baseline and for
	medium dataset sizes, but it is a known scalability limitation for larger datasets.

	\section{BoW}
	\subsection{Objective}
	While HOG focuses on global gradient structure, BoW was introduced to capture local visual
	patterns through keypoints. The goal is to convert a variable number of local ORB
	descriptors into a fixed-size global representation, so images can be compared in a compact
	and uniform way.

	In other words, BoW bridges local detail and global matching: it keeps the discriminative
	power of local descriptors but produces one standardized vector per image.

	\subsection{Code Structure Decision}
	BoW logic is split into:
	\begin{itemize}
		\item \texttt{BoWExtractor} (\texttt{include/bow.h}, \texttt{src/bow.cpp}) for vocabulary building, histogram extraction, and histogram distance.
		\item \texttt{bow(...)} wrapper (\texttt{src/matching.cpp}) for train/test loop and prediction print.
	\end{itemize}
	
	This separation follows the same architectural rule used across the project:
	core algorithm in an extractor class, orchestration in a lightweight wrapper.
	This improves readability and allows future experiments (different vocabularies,
	different local features, different distance metrics) without touching application flow.

	The current version also simplifies the BoW extractor interface to three public operations:
	\texttt{buildVocabulary(...)}, \texttt{extract(...)}, and \texttt{matchDescriptors(...)}.
	Unused timing/keypoint getter APIs were removed to keep the code shorter and easier to
	maintain.

	\subsection{Pipeline Choices}
	The implemented BoW pipeline is:
	\begin{enumerate}
		\item extract ORB descriptors from all train images,
		\item convert descriptors to \texttt{CV\_32F},
		\item run k-means to build a visual vocabulary (default size: 20 words),
		\item assign each descriptor to the closest visual word,
		\item build one histogram per image and normalize it,
		\item compare test and train histograms with L2 distance.
	\end{enumerate}

	This pipeline is intentionally classical and explicit. Every stage can be inspected
	independently (descriptors, vocabulary, histogram), making debugging and incremental
	improvement easier for the team.
	
	\subsection{Why ORB + K-means}
	\begin{itemize}
		\item ORB is already present in the project and is computationally light.
		\item K-means gives a direct and standard way to form visual words.
		\item Fixed vocabulary keeps the implementation deterministic and easy to tune.
	\end{itemize}
	
	Choosing ORB also reduces integration friction, since ORB-based utilities were already
	available and familiar in the codebase. K-means, despite being simple, provides a clear
	semantic interpretation: each cluster center is treated as a visual word.
	
	\subsection{Histogram and Matching Formula}
	Each image is represented by a normalized histogram \(b(x)\in R^K\), where \(K\)
	is vocabulary size. Prediction follows:
	\[
	\hat{y}(x) = \arg\min_{i} \lVert b(x) - b(x_i^{train}) \rVert_2
	\]
	
	Normalization reduces sensitivity to raw keypoint count differences between images,
	so comparison focuses more on visual word distribution than on absolute descriptor count.
	This is especially useful when images produce very different numbers of detected keypoints.

	\subsection{Robustness Choices}
	\begin{itemize}
		\item Vocabulary availability check before extraction.
		\item Empty descriptor/histogram checks.
		\item Skip logic in \texttt{bow(...)} when \texttt{extract(...)} returns \texttt{false}.
	\end{itemize}

	\subsection{Trade-off}
	The current implementation favors clarity over optimization:
	\begin{itemize}
		\item vocabulary is rebuilt at each run,
		\item nearest-neighbor matching is exhaustive,
		\item no TF-IDF or advanced scoring yet.
	\end{itemize}

	These trade-offs are deliberate at this stage: the code remains concise and easy to reason
	about, and it provides a reliable baseline before introducing acceleration or more advanced
	weighting schemes.

	\section{Metrics implementation}
	\subsection{Code Structure Decision}
	The metrics system is organized into three components:
	\begin{itemize}
		\item \texttt{metrics.h/cpp} (\texttt{include/metrics.h}, \texttt{src/metrics.cpp}) for 
		core data structures and computation functions.
		\item \texttt{print\_stats.h/cpp} (\texttt{include/print\_stats.h}, \texttt{src/print\_stats.cpp}) 
		for formatted output and reporting.
		\item \texttt{Metrics} struct as the central data container, passed by reference throughout 
		the pipeline.
	\end{itemize}

	This separation permits modular development: \texttt{metrics.cpp} handles numerical computations 
	and data management, while \texttt{print\_stats.cpp} focuses exclusively on presentation formatting. 
	This design makes it easy to add new output formats without modifying the core metrics logic.

	\subsection{Core Data Structure}
	The \texttt{Metrics} struct encapsulates all evaluation data:
	\begin{verbatim}
		struct Metrics {
			int num_classes;
			int total_samples;
			int correct_predictions;
			std::vector<std::vector<int>> confusion_matrix;
			std::vector<double> processing_times;
		};
	\end{verbatim}

	Key design decisions:
	\begin{itemize}
		\item \textbf{Confusion matrix}: Stored as \texttt{vector<vector<int>>} rather than a flat 
		array for intuitive indexing: \texttt{confusion\_matrix[true\_class][predicted\_class]}.
		\item \textbf{Processing times}: Stored as individual measurements rather than pre-computed 
		aggregates, allowing flexible statistical analysis (mean, median, percentiles) without data loss.
		\item \textbf{Counts vs rates}: Raw counts (\texttt{correct\_predictions}, \texttt{total\_samples}) 
		are stored; accuracy is computed on-demand to avoid synchronization issues during incremental updates.
	\end{itemize}

	\subsection{Accuracy Computation}
	Multiple accuracy metrics are supported:

	\textbf{Overall accuracy:}
	\[
	\text{accuracy} = \frac{\text{correct\_predictions}}{\text{total\_samples}}
	\]

	\textbf{Per-class accuracy:}
	\[
	\text{accuracy}_c = \frac{\text{confusion\_matrix}[c][c]}{\sum_j \text{confusion\_matrix}[c][j]}
	\]

	Per-class accuracy is computed by dividing the diagonal element (correct predictions for class \(c\)) 
	by the sum of the entire row (all predictions where the true class was \(c\)). This metric reveals 
	which classes are well-distinguished and which are frequently confused.

	The implementation includes proper handling of edge cases: zero-sample classes return 0.0 accuracy 
	rather than causing division by zero.

	\subsection{Timing Statistics}
	Processing time analysis provides four key metrics:
	\begin{itemize}
		\item \textbf{Total time}: Sum of all per-image processing times.
		\item \textbf{Mean time}: Average processing time per image.
		\item \textbf{Min/Max time}: Identifies best-case and worst-case performance.
	\end{itemize}

	These metrics are computed as:
	\[
	\text{mean} = \frac{1}{n}\sum_{i=1}^{n} t_i
	\]
	\[
	\text{min} = \min_i t_i, \quad \text{max} = \max_i t_i
	\]

	\subsection{Reporting Functions}
	The \texttt{print\_stats} module provides hierarchical reporting:

	\textbf{Component functions:}
	\begin{itemize}
		\item \texttt{printConfusionMatrix(...)}: Formatted confusion matrix table.
		\item \texttt{printTimingStats(...)}: Processing time summary.
		\item \texttt{printPerClassAccuracy(...)}: Per-class accuracy breakdown.
	\end{itemize}

	\textbf{Composite function:}
	\begin{itemize}
		\item \texttt{printClassificationReport(...)}: Combines all three components into a comprehensive 
		summary with optional algorithm name header.
	\end{itemize}

	This modular structure allows flexible reporting: debugging sessions might print only timing stats, 
	while final evaluation uses the full classification report.

	\subsection{Design Patterns}
	Several design patterns enhance usability:

	\noindent\textbf{Const-correctness:}
	Query functions take \texttt{const Metrics\&} to communicate that they perform read-only operations:
	\begin{verbatim}
	double totalAccuracy(const Metrics& metrics);
	\end{verbatim}

	\noindent\textbf{Reference parameters:}
	Update functions take \texttt{Metrics\&} to enable efficient in-place modification:
	\begin{verbatim}
	void addPrediction(Metrics& metrics, int true_class, int predicted_class);
	\end{verbatim}

	\subsection{Extensibility}
	The metrics system is designed for easy extension:
	\begin{itemize}
		\item Adding new metrics: Extend the \texttt{Metrics} struct and add corresponding computation 
		functions.
		\item New output formats: Add new functions to \texttt{print\_stats.cpp} or create separate 
		modules.
		\item Statistical tests: Raw data in \texttt{processing\_times} vector enables computation 
		of variance, standard deviation, percentiles, or hypothesis tests.
	\end{itemize}

	\subsection{Performance Considerations}
	The metrics system adds minimal overhead:
	\begin{itemize}
		\item Per-prediction update: \(O(1)\) for counter increments and confusion matrix access.
		\item Time recording: \(O(1)\) vector push\_back operation.
		\item Accuracy computation: \(O(c^2)\) where \(c\) is number of classes, negligible compared to feature extraction time.
	\end{itemize}

	\subsection{Integration with Feature Extractors}
	All feature extraction modules (SIFT, SURF, TM, HOG, BoW) use this common metrics infrastructure, ensuring:
	\begin{itemize}
		\item Consistent evaluation methodology across algorithms.
		\item Comparable timing measurements (all include feature extraction through final prediction).
		\item Uniform reporting format for easy comparison.
	\end{itemize}

	This standardization is critical for fair performance comparisons and for making informed decisions about which algorithm is best suited for the flower classification task.

	\section{Flower Health Detection}
	\textit{Section reserved for flower health detection details (to be completed).}

	\section{Performance Evaluation}
	\textit{Section reserved for metrics, confusion matrix, and timing analysis (to be completed).}

	\section{Conclusion}
	In this iteration, HOG and BoW were integrated as simple and modular baselines.
	The main design decision was to keep a consistent extractor interface and move
	algorithm-level loops outside \texttt{main.cpp} into a dedicated matching module.
	The latest revision further reduced complexity by removing unused state from HOG/BoW
	extractors and retaining only the operations actually used by the pipeline.
	
	\section{Work Distribution}

	The project workload was distributed among team members in the following way:
	\begin{itemize}
		\item Marco Carraro: SIFT, SURF and metrics system.
		\item Luca Pellegrini: preprocessing pipeline image container infrastructure, template matching.
		\item Francesco Vezzani: HOG and BoW implementation, matching wrappers,
	\end{itemize}

	All team members participated in code reviews, integration testing, and collaborative debugging sessions to ensure consistency across modules and resolve interface compatibility issues.
	
	\begin{table}[H]
	\centering
	\begin{tabular}{@{}lccc@{}}
	\toprule
	\textbf{Hours worked} & \textbf{Marco Carraro} & \textbf{Luca Pellegrini} & \textbf{Francesco Vezzani} \\ 
	\midrule
	\textbf{Total} & \textbf{$\sim$ 40} & \textbf{XX} & \textbf{XX} \\
	\bottomrule
	\end{tabular}
	\caption{Approximate hours worked per team member}
	
	\end{table}
	
	\section*{References / Links}
	
	\begin{itemize}
		\item Project repository: \url{https://github.com/kekko7072/Final_Project_Kernel_Rebooters/tree/main}
		\item OpenCV SIFT documentation: \url{https://docs.opencv.org/4.5.4/d7/d60/classcv_1_1SIFT.html#a94ee0141f77675822e574bbd2c079811}
		\item OpenCV SURF documentation: \url{https://docs.opencv.org/4.6.0/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html}
		\item OpenCV FlannBasedMatcher documentation: \url{https://docs.opencv.org/4.5.4/dc/de2/classcv_1_1FlannBasedMatcher.html}
		\item OpenCV HOGDescriptor documentation: \url{https://docs.opencv.org/4.x/d5/d33/structcv_1_1HOGDescriptor.html}
		\item OpenCV ORB documentation: \url{https://docs.opencv.org/4.x/db/d95/classcv_1_1ORB.html}
		\item OpenCV kmeans documentation: \url{https://docs.opencv.org/4.x/d1/d5c/tutorial_py_kmeans_opencv.html}
	\end{itemize}
	
\end{document}
